{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     2,
     34,
     43,
     60,
     73,
     77,
     98
    ]
   },
   "outputs": [],
   "source": [
    "class WriteForMe:\n",
    "    \n",
    "    def __init__(self, text, epochs=1, batchSize=50, seqLength=10, units=400, starter=\"\"):\n",
    "        '''\n",
    "        text: string of traning data\n",
    "        mapType: whether to map characters or words\n",
    "        \n",
    "        '''\n",
    "        self.batchSize = batchSize\n",
    "        self.epochs = epochs\n",
    "        self.text = text\n",
    "        \n",
    "        mapping = self.mapping()\n",
    "        self.nToKey = mapping[0]\n",
    "        self.keyToN = mapping[1]\n",
    "        \n",
    "        if len(starter) > 0:\n",
    "            print('starter given')\n",
    "            starter = self.encodeStr(starter)\n",
    "            self.seqLen = len(starter)\n",
    "            self.starter = starter\n",
    "        else:\n",
    "            self.seqLen = seqLength\n",
    "        \n",
    "        trainSet = self.preprocessing()\n",
    "        self.trainX = trainSet[0]\n",
    "        self.trainY = trainSet[1]\n",
    "        if len(starter) < 1:\n",
    "            print('default starter')\n",
    "            self.starter = trainSet[2]\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.modeling(units=units)\n",
    "\n",
    "    def mapping(self):\n",
    "        print('mapping')\n",
    "        \n",
    "        characters = sorted(list(set(self.text)))\n",
    "        self.chars = characters\n",
    "        nToChar = { n:char for n, char in enumerate(characters) }\n",
    "        charToN = { char:n for n, char in enumerate(characters) }\n",
    "        return [nToChar, charToN]\n",
    "\n",
    "    def preprocessing(self):\n",
    "        print('preprocessing')\n",
    "        \n",
    "        x = []\n",
    "        y = []\n",
    "        length = len(self.text)\n",
    "        for i in range(0, length-self.seqLen, 1):\n",
    "            sequence = self.text[i:i + self.seqLen]\n",
    "            label = self.text[i + self.seqLen]\n",
    "            x.append([self.keyToN[char] for char in sequence])\n",
    "            y.append(self.keyToN[label])\n",
    "            \n",
    "        xMod = np.reshape(x, (len(x), self.seqLen, 1))\n",
    "        xMod = xMod / float(len(self.keyToN))\n",
    "        yMod = np_utils.to_categorical(y)\n",
    "        return [xMod, yMod, x[self.seqLen-1]]\n",
    "    \n",
    "    def modeling(self, units):\n",
    "        print('modeling')\n",
    "        \n",
    "        self.model.add(LSTM(units, input_shape=(self.trainX.shape[1], self.trainX.shape[2]), return_sequences=True))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(LSTM(units, return_sequences=True))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(LSTM(units))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(self.trainY.shape[1], activation='softmax'))\n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        \n",
    "    def fit(self):\n",
    "        print('fitting')\n",
    "        self.model.fit(self.trainX, self.trainY, epochs=self.epochs, batch_size=self.batchSize)\n",
    "        \n",
    "    def generateText(self, length=400):\n",
    "        mappedStr = self.starter\n",
    "        print('mapped', mappedStr)\n",
    "        fullStr = [self.nToKey[val] for val in mappedStr]\n",
    "        print(fullStr)\n",
    "        \n",
    "        for i in range(length):\n",
    "            x = np.reshape(mappedStr, (1, len(mappedStr), 1))\n",
    "            x = x / float(len(self.chars))\n",
    "            \n",
    "            nextPred = np.argmax(self.model.predict(x, verbose=0))\n",
    "            fullStr.append(self.nToKey[nextPred])\n",
    "            mappedStr.append(nextPred)\n",
    "            mappedStr = mappedStr[1:]\n",
    "            \n",
    "        print(fullStr)\n",
    "        final = \"\"\n",
    "        for char in fullStr:\n",
    "            final += char\n",
    "        print(final)\n",
    "        \n",
    "    def encodeStr(self, string):\n",
    "        print('encoding')\n",
    "        return [self.keyToN[char] for char in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"data/emPosts.txt\", encoding=\"utf-8-sig\").read()\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model 07</h1>\n",
    "\n",
    "- **LSTM Units:** 512\n",
    "- **Epochs:** 10\n",
    "- **Batch Size:** 150\n",
    "- **Sequence Length:** 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 512\n",
    "epochs = 50\n",
    "batchSize = 150\n",
    "sequenceLength = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a _quarter_ of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486646 characters\n",
      "86731 words\n",
      "\n",
      "cutting data\n",
      "\n",
      "60830 characters\n",
      "10911 words\n"
     ]
    }
   ],
   "source": [
    "print(len(text), 'characters')\n",
    "print(len(text.split()), 'words')\n",
    "\n",
    "# Portioning text for faster testsing\n",
    "print('\\ncutting data\\n')\n",
    "cut = int(len(text) / 8)\n",
    "text = text[:cut]\n",
    "\n",
    "print(len(text), 'characters')\n",
    "print(len(text.split()), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping\n",
      "preprocessing\n",
      "default starter\n",
      "modeling\n"
     ]
    }
   ],
   "source": [
    "test07 = WriteForMe(text, epochs=epochs,\n",
    "                    batchSize=batchSize,\n",
    "                    seqLength=sequenceLength,\n",
    "                    units=units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "Epoch 1/50\n",
      "60730/60730 [==============================] - 2246s 37ms/step - loss: 2.9837\n",
      "Epoch 2/50\n",
      "60730/60730 [==============================] - 1874s 31ms/step - loss: 2.8562\n",
      "Epoch 3/50\n",
      "60730/60730 [==============================] - 1917s 32ms/step - loss: 2.6294\n",
      "Epoch 4/50\n",
      "60730/60730 [==============================] - 1873s 31ms/step - loss: 2.4504\n",
      "Epoch 5/50\n",
      "60730/60730 [==============================] - 1831s 30ms/step - loss: 2.2941\n",
      "Epoch 6/50\n",
      "60730/60730 [==============================] - 1819s 30ms/step - loss: 2.1758\n",
      "Epoch 7/50\n",
      "60730/60730 [==============================] - 1842s 30ms/step - loss: 2.0740\n",
      "Epoch 8/50\n",
      "60730/60730 [==============================] - 1874s 31ms/step - loss: 1.9914\n",
      "Epoch 9/50\n",
      "60730/60730 [==============================] - 1831s 30ms/step - loss: 1.9116\n",
      "Epoch 10/50\n",
      "60730/60730 [==============================] - 1879s 31ms/step - loss: 1.8354\n",
      "Epoch 11/50\n",
      "60730/60730 [==============================] - 1849s 30ms/step - loss: 1.7645\n",
      "Epoch 12/50\n",
      "60730/60730 [==============================] - 1815s 30ms/step - loss: 1.6998\n",
      "Epoch 13/50\n",
      "60730/60730 [==============================] - 1881s 31ms/step - loss: 1.6321\n",
      "Epoch 14/50\n",
      "60730/60730 [==============================] - 1886s 31ms/step - loss: 1.5724\n",
      "Epoch 15/50\n",
      "60730/60730 [==============================] - 1910s 31ms/step - loss: 1.5010\n",
      "Epoch 16/50\n",
      "60730/60730 [==============================] - 1911s 31ms/step - loss: 1.4369\n",
      "Epoch 17/50\n",
      "60730/60730 [==============================] - 1848s 30ms/step - loss: 1.3748\n",
      "Epoch 18/50\n",
      "60730/60730 [==============================] - 1905s 31ms/step - loss: 1.3102\n",
      "Epoch 19/50\n",
      "60730/60730 [==============================] - 1909s 31ms/step - loss: 1.2572\n",
      "Epoch 20/50\n",
      "60730/60730 [==============================] - 1889s 31ms/step - loss: 1.1935\n",
      "Epoch 21/50\n",
      "60730/60730 [==============================] - 1893s 31ms/step - loss: 1.1346\n",
      "Epoch 22/50\n",
      "60730/60730 [==============================] - 1933s 32ms/step - loss: 1.0738\n",
      "Epoch 23/50\n",
      "60730/60730 [==============================] - 1951s 32ms/step - loss: 1.0203\n",
      "Epoch 24/50\n",
      "60730/60730 [==============================] - 1977s 33ms/step - loss: 0.9745\n",
      "Epoch 25/50\n",
      "60730/60730 [==============================] - 1962s 32ms/step - loss: 0.9198\n",
      "Epoch 26/50\n",
      "60730/60730 [==============================] - 1958s 32ms/step - loss: 0.8719\n",
      "Epoch 27/50\n",
      "60730/60730 [==============================] - 1975s 33ms/step - loss: 0.8227\n",
      "Epoch 28/50\n",
      "60730/60730 [==============================] - 2011s 33ms/step - loss: 0.7806\n",
      "Epoch 29/50\n",
      "60730/60730 [==============================] - 2046s 34ms/step - loss: 0.7345\n",
      "Epoch 30/50\n",
      "60730/60730 [==============================] - 2047s 34ms/step - loss: 0.6938\n",
      "Epoch 31/50\n",
      "60730/60730 [==============================] - 2094s 34ms/step - loss: 0.6516\n",
      "Epoch 32/50\n",
      "60730/60730 [==============================] - 2158s 36ms/step - loss: 0.6240\n",
      "Epoch 33/50\n",
      "60730/60730 [==============================] - 2157s 36ms/step - loss: 0.5953\n",
      "Epoch 34/50\n",
      "60730/60730 [==============================] - 2132s 35ms/step - loss: 0.5634\n",
      "Epoch 35/50\n",
      "60730/60730 [==============================] - 2070s 34ms/step - loss: 0.5280\n",
      "Epoch 36/50\n",
      "60730/60730 [==============================] - 2068s 34ms/step - loss: 0.5082\n",
      "Epoch 37/50\n",
      "60730/60730 [==============================] - 2082s 34ms/step - loss: 0.4825\n",
      "Epoch 38/50\n",
      "60730/60730 [==============================] - 2127s 35ms/step - loss: 0.4552\n",
      "Epoch 39/50\n",
      "60730/60730 [==============================] - 2227s 37ms/step - loss: 0.4366\n",
      "Epoch 40/50\n",
      "60730/60730 [==============================] - 2141s 35ms/step - loss: 0.4181\n",
      "Epoch 41/50\n",
      "60730/60730 [==============================] - 2132s 35ms/step - loss: 0.3928\n",
      "Epoch 42/50\n",
      "60730/60730 [==============================] - 2160s 36ms/step - loss: 0.3752\n",
      "Epoch 43/50\n",
      "60730/60730 [==============================] - 2209s 36ms/step - loss: 0.3713\n",
      "Epoch 44/50\n",
      "60730/60730 [==============================] - 2183s 36ms/step - loss: 0.3489\n",
      "Epoch 45/50\n",
      "60730/60730 [==============================] - 2211s 36ms/step - loss: 0.3446\n",
      "Epoch 46/50\n",
      "60730/60730 [==============================] - 2159s 36ms/step - loss: 0.3275\n",
      "Epoch 47/50\n",
      "60730/60730 [==============================] - 2193s 36ms/step - loss: 0.3139\n",
      "Epoch 48/50\n",
      "60730/60730 [==============================] - 2240s 37ms/step - loss: 0.3063\n",
      "Epoch 49/50\n",
      "60730/60730 [==============================] - 2159s 36ms/step - loss: 0.2905\n",
      "Epoch 50/50\n",
      "60730/60730 [==============================] - 2151s 35ms/step - loss: 0.2841\n"
     ]
    }
   ],
   "source": [
    "test07.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapped [34, 34, 20, 19, 1, 17, 40, 1, 35, 23, 20, 28, 1, 38, 24, 35, 23, 9, 1, 35, 23, 20, 24, 33, 1, 38, 16, 33, 1, 38, 24, 35, 23, 1, 19, 24, 16, 28, 30, 29, 19, 1, 23, 16, 19, 1, 18, 20, 16, 34, 20, 19, 7, 1, 40, 20, 35, 1, 35, 36, 33, 28, 30, 24, 27, 1, 33, 20, 28, 16, 24, 29, 20, 19, 1, 38, 24, 35, 23, 24, 29, 1, 18, 27, 30, 37, 20, 33, 1, 38, 24, 35, 23, 1, 35, 23, 20, 1, 19, 20]\n",
      "['s', 's', 'e', 'd', ' ', 'b', 'y', ' ', 't', 'h', 'e', 'm', ' ', 'w', 'i', 't', 'h', '.', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'w', 'a', 'r', ' ', 'w', 'i', 't', 'h', ' ', 'd', 'i', 'a', 'm', 'o', 'n', 'd', ' ', 'h', 'a', 'd', ' ', 'c', 'e', 'a', 's', 'e', 'd', ',', ' ', 'y', 'e', 't', ' ', 't', 'u', 'r', 'm', 'o', 'i', 'l', ' ', 'r', 'e', 'm', 'a', 'i', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', 'i', 'n', ' ', 'c', 'l', 'o', 'v', 'e', 'r', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'd', 'e']\n",
      "['s', 's', 'e', 'd', ' ', 'b', 'y', ' ', 't', 'h', 'e', 'm', ' ', 'w', 'i', 't', 'h', '.', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'w', 'a', 'r', ' ', 'w', 'i', 't', 'h', ' ', 'd', 'i', 'a', 'm', 'o', 'n', 'd', ' ', 'h', 'a', 'd', ' ', 'c', 'e', 'a', 's', 'e', 'd', ',', ' ', 'y', 'e', 't', ' ', 't', 'u', 'r', 'm', 'o', 'i', 'l', ' ', 'r', 'e', 'm', 'a', 'i', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', 'i', 'n', ' ', 'c', 'l', 'o', 'v', 'e', 'r', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'd', 'e', 'a', 't', 'h', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'b', 'r', 'i', 'a', 'r', ' ', 'c', 'o', 'w', 'n', 's', ' ', 'a', 'n', 'd', ' ', 'a', 'r', 'r', 'a', 'c', 'k', ' ', 'a', 't', ' ', 'n', 'e', 'a', 's', 't', ' ', 'o', 'n', 'e', ' ', 'o', 'u', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'g', 'r', 'o', 'u', 'p', '.', ' ', 'h', 'e', ' ', 'c', 'o', 'u', 'l', 'd', 'n', \"'\", 't', ' ', 's', 'a', 'y', ' ', 'n', 'o', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'e', 'l', 'f', ' ', 't', 'h', 'e', 'y', ' ', 'm', 'u', 't', 'u', 'a', 'l', 'l', 'y', ' ', 'e', 'i', 't', 'h', 'e', 'r', ',', ' ', 'k', 'n', 'o', 'w', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', ' ', 'l', 't', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 'a', 't', ' ', 'l', 'e', 'a', 's', 't', ' ', 'b', 'e', ' ', 'a', 'c', 'c', 'e', 'p', 't', 'e', 'd', ' ', 'f', 'o', 'r', ' ', 'l', 'i', 'a', 'n', 'a', '.', ' ', 'i', 't', ' ', 'w', 'a', 's', ' ', 'w', 'i', 't', 'h', ' ', 's', 'u', 'c', 'h', ' ', 'a', 'b', 'u', 'n', 'd', 'a', 'n', 't', ' ', 'r', 'e', 'l', 'i', 'e', 'f', ' ', 't', 'h', 'a', 't', ' ', 's', 'h', 'e', ' ', 'l', 'e', 'a', 'n', 'e', 'd', ' ', 'a', 'g', 'a', 'i', 'n', 's', 't', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'm', 'a', 'n', 'e', ' ', 'l', 'i', 'f', 't', 'e', 'd', ' ', 'a', 'n', ' ', 'a', 'r', 'm', ' ', 't', 'o', ' ', 'w', 'r', 'a', 'p', ' ', 'a', 'r', 'o', 'u', 'n', 'd', ' ', 't', 'h', 'e', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'h', 'o', 'u', 'l', 'd', 'e', 'r', 's', ' ', 'i', 'n', ' ', 'h', 'a', 'l', 'f', ' ', 'a', 'n', ' ', 'e', 'a', 'u', 'n', 'i', 'e', 'r', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 's', 'i', 'z', 'a', 'b', 'l', 'e', ' ', 's', 'i', 'd', 'e', '.', ' ', '”', 'a', 'n', 'y', 'i', 'n', 'g', ' ', 'w', 'i', 'n', 'l', 'a', 'g', 'e', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', 's', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'p', 'a', 's', 't', ' ', 't', 'w', 'o', ' ', 'a', 'l', 't', 'a', 'r', 's', ',', ' ']\n",
      "ssed by them with. their war with diamond had ceased, yet turmoil remained within clover with the death of the briar cowns and arrack at neast one out of the group. he couldn't say no to the elf they mutually either, knowing that lt could at least be accepted for liana. it was with such abundant relief that she leaned against him, and the mane lifted an arm to wrap around the smaller shoulders in half an eaunier with a sizable side. ”anying winlage with their experiences of the past two altars, \n"
     ]
    }
   ],
   "source": [
    "test07.generateText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
