{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     34,
     43,
     73,
     77,
     98
    ]
   },
   "outputs": [],
   "source": [
    "class WriteForMe:\n",
    "    \n",
    "    def __init__(self, text, epochs=1, batchSize=50, seqLength=10, units=400, starter=\"\"):\n",
    "        '''\n",
    "        text: string of traning data\n",
    "        mapType: whether to map characters or words\n",
    "        \n",
    "        '''\n",
    "        self.batchSize = batchSize\n",
    "        self.epochs = epochs\n",
    "        self.text = text\n",
    "        \n",
    "        mapping = self.mapping()\n",
    "        self.nToKey = mapping[0]\n",
    "        self.keyToN = mapping[1]\n",
    "        \n",
    "        if len(starter) > 0:\n",
    "            print('starter given')\n",
    "            starter = self.encodeStr(starter)\n",
    "            self.seqLen = len(starter)\n",
    "            self.starter = starter\n",
    "        else:\n",
    "            self.seqLen = seqLength\n",
    "        \n",
    "        trainSet = self.preprocessing()\n",
    "        self.trainX = trainSet[0]\n",
    "        self.trainY = trainSet[1]\n",
    "        if len(starter) < 1:\n",
    "            print('default starter')\n",
    "            self.starter = trainSet[2]\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.modeling(units=units)\n",
    "\n",
    "    def mapping(self):\n",
    "        print('mapping')\n",
    "        \n",
    "        characters = sorted(list(set(self.text)))\n",
    "        self.chars = characters\n",
    "        nToChar = { n:char for n, char in enumerate(characters) }\n",
    "        charToN = { char:n for n, char in enumerate(characters) }\n",
    "        return [nToChar, charToN]\n",
    "\n",
    "    def preprocessing(self):\n",
    "        print('preprocessing')\n",
    "        \n",
    "        x = []\n",
    "        y = []\n",
    "        length = len(self.text)\n",
    "        for i in range(0, length-self.seqLen, 1):\n",
    "            sequence = self.text[i:i + self.seqLen]\n",
    "            label = self.text[i + self.seqLen]\n",
    "            x.append([self.keyToN[char] for char in sequence])\n",
    "            y.append(self.keyToN[label])\n",
    "            \n",
    "        xMod = np.reshape(x, (len(x), self.seqLen, 1))\n",
    "        xMod = xMod / float(len(self.keyToN))\n",
    "        yMod = np_utils.to_categorical(y)\n",
    "        return [xMod, yMod, x[self.seqLen-1]]\n",
    "    \n",
    "    def modeling(self, units):\n",
    "        print('modeling')\n",
    "        \n",
    "        self.model.add(LSTM(units, input_shape=(self.trainX.shape[1], self.trainX.shape[2]), return_sequences=True))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        \n",
    "        self.model.add(LSTM(units, return_sequences=True))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        \n",
    "        self.model.add(LSTM(units, return_sequences=True))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        \n",
    "        self.model.add(LSTM(units))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        \n",
    "        self.model.add(Dense(self.trainY.shape[1], activation='softmax'))\n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        \n",
    "    def fit(self):\n",
    "        print('fitting')\n",
    "        self.model.fit(self.trainX, self.trainY, epochs=self.epochs, batch_size=self.batchSize)\n",
    "        \n",
    "    def generateText(self, length=400):\n",
    "        mappedStr = self.starter\n",
    "        print('mapped', mappedStr)\n",
    "        fullStr = [self.nToKey[val] for val in mappedStr]\n",
    "        print(fullStr)\n",
    "        \n",
    "        for i in range(length):\n",
    "            x = np.reshape(mappedStr, (1, len(mappedStr), 1))\n",
    "            x = x / float(len(self.chars))\n",
    "            \n",
    "            nextPred = np.argmax(self.model.predict(x, verbose=0))\n",
    "            fullStr.append(self.nToKey[nextPred])\n",
    "            mappedStr.append(nextPred)\n",
    "            mappedStr = mappedStr[1:]\n",
    "            \n",
    "        print(fullStr)\n",
    "        final = \"\"\n",
    "        for char in fullStr:\n",
    "            final += char\n",
    "        print(final)\n",
    "        \n",
    "    def encodeStr(self, string):\n",
    "        print('encoding')\n",
    "        return [self.keyToN[char] for char in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"data/emPosts.txt\", encoding=\"utf-8-sig\").read()\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model 05</h1>\n",
    "\n",
    "- **LSTM Units:** 256\n",
    "- **Epochs:** 1\n",
    "- **Batch Size:** 100\n",
    "- **Sequence Length:** 100\n",
    "- +1 additional LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 256\n",
    "epochs = 1\n",
    "batchSize = 100\n",
    "sequenceLength = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a _quarter_ of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486646 characters\n",
      "86731 words\n",
      "\n",
      "cutting data\n",
      "\n",
      "121661 characters\n",
      "21821 words\n"
     ]
    }
   ],
   "source": [
    "print(len(text), 'characters')\n",
    "print(len(text.split()), 'words')\n",
    "\n",
    "# Portioning text for faster testsing\n",
    "print('\\ncutting data\\n')\n",
    "cut = int(len(text) / 4)\n",
    "text = text[:cut]\n",
    "\n",
    "print(len(text), 'characters')\n",
    "print(len(text.split()), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping\n",
      "preprocessing\n",
      "default starter\n",
      "modeling\n"
     ]
    }
   ],
   "source": [
    "test05 = WriteForMe(text, epochs=epochs,\n",
    "                    batchSize=batchSize,\n",
    "                    seqLength=sequenceLength,\n",
    "                    units=units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "Epoch 1/1\n",
      "121561/121561 [==============================] - 6435s 53ms/step - loss: 2.9773\n"
     ]
    }
   ],
   "source": [
    "test05.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longest _observed_ ETA: ~ **1:30:00** per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapped [37, 37, 23, 22, 1, 20, 43, 1, 38, 26, 23, 31, 1, 41, 27, 38, 26, 9, 1, 38, 26, 23, 27, 36, 1, 41, 19, 36, 1, 41, 27, 38, 26, 1, 22, 27, 19, 31, 33, 32, 22, 1, 26, 19, 22, 1, 21, 23, 19, 37, 23, 22, 7, 1, 43, 23, 38, 1, 38, 39, 36, 31, 33, 27, 30, 1, 36, 23, 31, 19, 27, 32, 23, 22, 1, 41, 27, 38, 26, 27, 32, 1, 21, 30, 33, 40, 23, 36, 1, 41, 27, 38, 26, 1, 38, 26, 23, 1, 22, 23]\n",
      "['s', 's', 'e', 'd', ' ', 'b', 'y', ' ', 't', 'h', 'e', 'm', ' ', 'w', 'i', 't', 'h', '.', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'w', 'a', 'r', ' ', 'w', 'i', 't', 'h', ' ', 'd', 'i', 'a', 'm', 'o', 'n', 'd', ' ', 'h', 'a', 'd', ' ', 'c', 'e', 'a', 's', 'e', 'd', ',', ' ', 'y', 'e', 't', ' ', 't', 'u', 'r', 'm', 'o', 'i', 'l', ' ', 'r', 'e', 'm', 'a', 'i', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', 'i', 'n', ' ', 'c', 'l', 'o', 'v', 'e', 'r', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'd', 'e']\n",
      "['s', 's', 'e', 'd', ' ', 'b', 'y', ' ', 't', 'h', 'e', 'm', ' ', 'w', 'i', 't', 'h', '.', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'w', 'a', 'r', ' ', 'w', 'i', 't', 'h', ' ', 'd', 'i', 'a', 'm', 'o', 'n', 'd', ' ', 'h', 'a', 'd', ' ', 'c', 'e', 'a', 's', 'e', 'd', ',', ' ', 'y', 'e', 't', ' ', 't', 'u', 'r', 'm', 'o', 'i', 'l', ' ', 'r', 'e', 'm', 'a', 'i', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', 'i', 'n', ' ', 'c', 'l', 'o', 'v', 'e', 'r', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'd', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "ssed by them with. their war with diamond had ceased, yet turmoil remained within clover with the de                                                                                                                                                                                                                                                                                                                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "test05.generateText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
