{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     34,
     43,
     60,
     73,
     77,
     98
    ]
   },
   "outputs": [],
   "source": [
    "class WriteForMe:\n",
    "    \n",
    "    def __init__(self, text, epochs=1, batchSize=50, seqLength=10, units=400, starter=\"\"):\n",
    "        '''\n",
    "        text: string of traning data\n",
    "        mapType: whether to map characters or words\n",
    "        \n",
    "        '''\n",
    "        self.batchSize = batchSize\n",
    "        self.epochs = epochs\n",
    "        self.text = text\n",
    "        \n",
    "        mapping = self.mapping()\n",
    "        self.nToKey = mapping[0]\n",
    "        self.keyToN = mapping[1]\n",
    "        \n",
    "        if len(starter) > 0:\n",
    "            print('starter given')\n",
    "            starter = self.encodeStr(starter)\n",
    "            self.seqLen = len(starter)\n",
    "            self.starter = starter\n",
    "        else:\n",
    "            self.seqLen = seqLength\n",
    "        \n",
    "        trainSet = self.preprocessing()\n",
    "        self.trainX = trainSet[0]\n",
    "        self.trainY = trainSet[1]\n",
    "        if len(starter) < 1:\n",
    "            print('default starter')\n",
    "            self.starter = trainSet[2]\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.modeling(units=units)\n",
    "\n",
    "    def mapping(self):\n",
    "        print('mapping')\n",
    "        \n",
    "        characters = sorted(list(set(self.text)))\n",
    "        self.chars = characters\n",
    "        nToChar = { n:char for n, char in enumerate(characters) }\n",
    "        charToN = { char:n for n, char in enumerate(characters) }\n",
    "        return [nToChar, charToN]\n",
    "\n",
    "    def preprocessing(self):\n",
    "        print('preprocessing')\n",
    "        \n",
    "        x = []\n",
    "        y = []\n",
    "        length = len(self.text)\n",
    "        for i in range(0, length-self.seqLen, 1):\n",
    "            sequence = self.text[i:i + self.seqLen]\n",
    "            label = self.text[i + self.seqLen]\n",
    "            x.append([self.keyToN[char] for char in sequence])\n",
    "            y.append(self.keyToN[label])\n",
    "            \n",
    "        xMod = np.reshape(x, (len(x), self.seqLen, 1))\n",
    "        xMod = xMod / float(len(self.keyToN))\n",
    "        yMod = np_utils.to_categorical(y)\n",
    "        return [xMod, yMod, x[self.seqLen-1]]\n",
    "    \n",
    "    def modeling(self, units):\n",
    "        print('modeling')\n",
    "        \n",
    "        self.model.add(LSTM(units, input_shape=(self.trainX.shape[1], self.trainX.shape[2]), return_sequences=True))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(LSTM(units, return_sequences=True))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(LSTM(units))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(self.trainY.shape[1], activation='softmax'))\n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        \n",
    "    def fit(self):\n",
    "        print('fitting')\n",
    "        self.model.fit(self.trainX, self.trainY, epochs=self.epochs, batch_size=self.batchSize)\n",
    "        \n",
    "    def generateText(self, length=400):\n",
    "        mappedStr = self.starter\n",
    "        print('mapped', mappedStr)\n",
    "        fullStr = [self.nToKey[val] for val in mappedStr]\n",
    "        print(fullStr)\n",
    "        \n",
    "        for i in range(length):\n",
    "            x = np.reshape(mappedStr, (1, len(mappedStr), 1))\n",
    "            x = x / float(len(self.chars))\n",
    "            \n",
    "            nextPred = np.argmax(self.model.predict(x, verbose=0))\n",
    "            fullStr.append(self.nToKey[nextPred])\n",
    "            mappedStr.append(nextPred)\n",
    "            mappedStr = mappedStr[1:]\n",
    "            \n",
    "        print(fullStr)\n",
    "        final = \"\"\n",
    "        for char in fullStr:\n",
    "            final += char\n",
    "        print(final)\n",
    "        \n",
    "    def encodeStr(self, string):\n",
    "        print('encoding')\n",
    "        return [self.keyToN[char] for char in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"data/emPosts.txt\", encoding=\"utf-8-sig\").read()\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model 02</h1>\n",
    "\n",
    "- **LSTM Units:** 700\n",
    "- **Epochs:** 10\n",
    "- **Batch Size:** 100\n",
    "- **Sequence Length:** 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 700\n",
    "epochs = 10\n",
    "batchSize = 100\n",
    "sequenceLength = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a _quarter_ of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486646 characters\n",
      "86731 words\n",
      "\n",
      "cutting data\n",
      "\n",
      "121661 characters\n",
      "21821 words\n"
     ]
    }
   ],
   "source": [
    "print(len(text), 'characters')\n",
    "print(len(text.split()), 'words')\n",
    "\n",
    "# Portioning text for faster testsing\n",
    "print('\\ncutting data\\n')\n",
    "cut = int(len(text) / 4)\n",
    "text = text[:cut]\n",
    "\n",
    "print(len(text), 'characters')\n",
    "print(len(text.split()), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping\n",
      "preprocessing\n",
      "default starter\n",
      "modeling\n"
     ]
    }
   ],
   "source": [
    "test02 = WriteForMe(text, epochs=epochs,\n",
    "                    batchSize=batchSize,\n",
    "                    seqLength=sequenceLength,\n",
    "                    units=units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "Epoch 1/10\n",
      "121561/121561 [==============================] - 9024s 74ms/step - loss: 2.7957\n",
      "Epoch 2/10\n",
      "121561/121561 [==============================] - 5364s 44ms/step - loss: 2.3990\n",
      "Epoch 3/10\n",
      "121561/121561 [==============================] - 5458s 45ms/step - loss: 2.1509\n",
      "Epoch 4/10\n",
      "121561/121561 [==============================] - 5164s 42ms/step - loss: 1.9878\n",
      "Epoch 5/10\n",
      "121561/121561 [==============================] - 5258s 43ms/step - loss: 1.8607\n",
      "Epoch 6/10\n",
      "121561/121561 [==============================] - 5261s 43ms/step - loss: 1.7561\n",
      "Epoch 7/10\n",
      "121561/121561 [==============================] - 6107s 50ms/step - loss: 1.6581\n",
      "Epoch 8/10\n",
      "121561/121561 [==============================] - 5733s 47ms/step - loss: 1.5714\n",
      "Epoch 9/10\n",
      "121561/121561 [==============================] - 5691s 47ms/step - loss: 1.4891\n",
      "Epoch 10/10\n",
      "121561/121561 [==============================] - 5586s 46ms/step - loss: 1.4106\n"
     ]
    }
   ],
   "source": [
    "test02.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longest _observed_ ETA: ~ **1:30:00** per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapped [37, 37, 23, 22, 1, 20, 43, 1, 38, 26, 23, 31, 1, 41, 27, 38, 26, 9, 1, 38, 26, 23, 27, 36, 1, 41, 19, 36, 1, 41, 27, 38, 26, 1, 22, 27, 19, 31, 33, 32, 22, 1, 26, 19, 22, 1, 21, 23, 19, 37, 23, 22, 7, 1, 43, 23, 38, 1, 38, 39, 36, 31, 33, 27, 30, 1, 36, 23, 31, 19, 27, 32, 23, 22, 1, 41, 27, 38, 26, 27, 32, 1, 21, 30, 33, 40, 23, 36, 1, 41, 27, 38, 26, 1, 38, 26, 23, 1, 22, 23]\n",
      "['s', 's', 'e', 'd', ' ', 'b', 'y', ' ', 't', 'h', 'e', 'm', ' ', 'w', 'i', 't', 'h', '.', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'w', 'a', 'r', ' ', 'w', 'i', 't', 'h', ' ', 'd', 'i', 'a', 'm', 'o', 'n', 'd', ' ', 'h', 'a', 'd', ' ', 'c', 'e', 'a', 's', 'e', 'd', ',', ' ', 'y', 'e', 't', ' ', 't', 'u', 'r', 'm', 'o', 'i', 'l', ' ', 'r', 'e', 'm', 'a', 'i', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', 'i', 'n', ' ', 'c', 'l', 'o', 'v', 'e', 'r', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'd', 'e']\n",
      "['s', 's', 'e', 'd', ' ', 'b', 'y', ' ', 't', 'h', 'e', 'm', ' ', 'w', 'i', 't', 'h', '.', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'w', 'a', 'r', ' ', 'w', 'i', 't', 'h', ' ', 'd', 'i', 'a', 'm', 'o', 'n', 'd', ' ', 'h', 'a', 'd', ' ', 'c', 'e', 'a', 's', 'e', 'd', ',', ' ', 'y', 'e', 't', ' ', 't', 'u', 'r', 'm', 'o', 'i', 'l', ' ', 'r', 'e', 'm', 'a', 'i', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', 'i', 'n', ' ', 'c', 'l', 'o', 'v', 'e', 'r', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'd', 'e', 's', 't', 'a', 'i', 'n', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 's', 'a', 'm', 'e', ' ', 'p', 'a', 's', 's', ' ', 't', 'h', 'a', 't', ' ', 'h', 'e', ' ', 's', 'u', 'a', 'r', 't', 'e', 'd', ' ', 'h', 'i', 'm', 's', 'e', 'l', 'f', ' ', 's', 'o', ' ', 'm', 'e', 'a', 'v', 'e', ',', ' ', 's', 'h', 'e', ' ', 's', 'h', 'o', 'r', 'n', ' ', 'w', 'a', 's', ' ', 't', 'h', 'e', ' ', 's', 'o', 'u', 'n', 'd', ' ', 'a', 'n', 'd', ' ', 'a', ' ', 'd', 'e', 'a', 't', 'h', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'e', 's', 't', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's']\n",
      "ssed by them with. their war with diamond had ceased, yet turmoil remained within clover with the destain and the same pass that he suarted himself so meave, she shorn was the sound and a death of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his own a peston of his\n"
     ]
    }
   ],
   "source": [
    "test02.generateText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
