{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Custom Model</h1>\n",
    "\n",
    "First test of the tutorial: https://www.analyticsvidhya.com/blog/2018/03/text-generation-using-python-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486646 characters\n",
      "12166 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(\"data/emPosts.txt\", encoding=\"utf-8-sig\").read()\n",
    "text = text.lower()\n",
    "print(len(text), 'characters')\n",
    "\n",
    "# Portioning text for faster testsing\n",
    "cut = int(len(text) / 40)\n",
    "print(cut, 'characters')\n",
    "text = text[:cut]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Mapping</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " \"'\": 4,\n",
       " ',': 5,\n",
       " '-': 6,\n",
       " '.': 7,\n",
       " '0': 8,\n",
       " '2': 9,\n",
       " ':': 10,\n",
       " '?': 11,\n",
       " 'a': 12,\n",
       " 'b': 13,\n",
       " 'c': 14,\n",
       " 'd': 15,\n",
       " 'e': 16,\n",
       " 'f': 17,\n",
       " 'g': 18,\n",
       " 'h': 19,\n",
       " 'i': 20,\n",
       " 'j': 21,\n",
       " 'k': 22,\n",
       " 'l': 23,\n",
       " 'm': 24,\n",
       " 'n': 25,\n",
       " 'o': 26,\n",
       " 'p': 27,\n",
       " 'q': 28,\n",
       " 'r': 29,\n",
       " 's': 30,\n",
       " 't': 31,\n",
       " 'u': 32,\n",
       " 'v': 33,\n",
       " 'w': 34,\n",
       " 'x': 35,\n",
       " 'y': 36,\n",
       " 'z': 37,\n",
       " '~': 38,\n",
       " 'â': 39,\n",
       " '’': 40,\n",
       " '“': 41,\n",
       " '”': 42,\n",
       " '€': 43}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping Characters\n",
    "\n",
    "characters = sorted(list(set(text)))\n",
    "nToChar = { n:char for n, char in enumerate(characters) }\n",
    "charToN = { char:n for n, char in enumerate(characters) }\n",
    "\n",
    "charToN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'a': 1,\n",
       " 'about': 2,\n",
       " 'above': 3,\n",
       " 'accepting': 4,\n",
       " 'accompanying': 5,\n",
       " 'accusingly': 6,\n",
       " 'achieving': 7,\n",
       " 'acknowledgement': 8,\n",
       " 'acquaintances': 9,\n",
       " 'acted': 10,\n",
       " 'adding': 11,\n",
       " 'adept': 12,\n",
       " 'adhered': 13,\n",
       " 'admit': 14,\n",
       " 'after': 15,\n",
       " 'afternoon': 16,\n",
       " 'against': 17,\n",
       " 'age': 18,\n",
       " 'agenda': 19,\n",
       " 'aimed': 20,\n",
       " 'air': 21,\n",
       " 'all': 22,\n",
       " 'almost': 23,\n",
       " 'along': 24,\n",
       " 'alove': 25,\n",
       " 'already': 26,\n",
       " 'also': 27,\n",
       " 'altar': 28,\n",
       " 'altars': 29,\n",
       " 'although': 30,\n",
       " 'always': 31,\n",
       " 'amongst': 32,\n",
       " 'an': 33,\n",
       " 'and': 34,\n",
       " 'animated': 35,\n",
       " 'anomalies': 36,\n",
       " 'anomaly': 37,\n",
       " 'another': 38,\n",
       " 'answer': 39,\n",
       " 'answered': 40,\n",
       " 'anymore': 41,\n",
       " 'anyone': 42,\n",
       " 'anything': 43,\n",
       " 'anyways': 44,\n",
       " 'apart': 45,\n",
       " 'appearance': 46,\n",
       " 'archives': 47,\n",
       " 'are': 48,\n",
       " 'arm': 49,\n",
       " 'arms': 50,\n",
       " 'aroma': 51,\n",
       " 'arose': 52,\n",
       " 'around': 53,\n",
       " 'arrived': 54,\n",
       " 'as': 55,\n",
       " 'asariel': 56,\n",
       " 'assumptions': 57,\n",
       " 'at': 58,\n",
       " 'atop': 59,\n",
       " 'attachments': 60,\n",
       " 'attack': 61,\n",
       " 'attempt': 62,\n",
       " 'attention': 63,\n",
       " 'audible': 64,\n",
       " 'aura': 65,\n",
       " 'awakened': 66,\n",
       " 'away': 67,\n",
       " 'back': 68,\n",
       " 'backpack': 69,\n",
       " 'bad': 70,\n",
       " 'bag': 71,\n",
       " 'barely': 72,\n",
       " 'barrier': 73,\n",
       " 'battle': 74,\n",
       " 'be': 75,\n",
       " 'beautiful': 76,\n",
       " 'because': 77,\n",
       " 'become': 78,\n",
       " 'bed': 79,\n",
       " 'been': 80,\n",
       " 'before': 81,\n",
       " 'behind': 82,\n",
       " 'beliefs': 83,\n",
       " 'belonged': 84,\n",
       " 'benches': 85,\n",
       " 'bend': 86,\n",
       " 'beneath': 87,\n",
       " 'bigger': 88,\n",
       " 'bile': 89,\n",
       " 'bite': 90,\n",
       " 'bitter': 91,\n",
       " 'blend': 92,\n",
       " 'blood': 93,\n",
       " 'bloody': 94,\n",
       " 'blooms': 95,\n",
       " 'blossoming': 96,\n",
       " 'bodies': 97,\n",
       " 'body': 98,\n",
       " 'book': 99,\n",
       " 'boring': 100,\n",
       " 'both': 101,\n",
       " 'brewing': 102,\n",
       " 'briar': 103,\n",
       " 'briars': 104,\n",
       " 'broad': 105,\n",
       " 'bss': 106,\n",
       " 'burnt': 107,\n",
       " 'burst': 108,\n",
       " 'but': 109,\n",
       " 'by': 110,\n",
       " 'cacao': 111,\n",
       " 'calm': 112,\n",
       " 'came': 113,\n",
       " 'can': 114,\n",
       " 'capital': 115,\n",
       " 'captain': 116,\n",
       " 'carded': 117,\n",
       " 'care': 118,\n",
       " 'cast': 119,\n",
       " 'casting': 120,\n",
       " 'castles': 121,\n",
       " 'caught': 122,\n",
       " 'cautious': 123,\n",
       " 'ceased': 124,\n",
       " 'chameleons': 125,\n",
       " 'change': 126,\n",
       " 'check': 127,\n",
       " 'checked': 128,\n",
       " 'chest': 129,\n",
       " 'chocolate': 130,\n",
       " 'church': 131,\n",
       " 'city': 132,\n",
       " 'clarity': 133,\n",
       " 'claws': 134,\n",
       " 'clear': 135,\n",
       " 'clearer': 136,\n",
       " 'clicked': 137,\n",
       " 'cloak': 138,\n",
       " 'close': 139,\n",
       " 'closer': 140,\n",
       " 'clover': 141,\n",
       " 'clovers': 142,\n",
       " 'cluster': 143,\n",
       " 'colleagues': 144,\n",
       " 'colorful': 145,\n",
       " 'coming': 146,\n",
       " 'compared': 147,\n",
       " 'completely': 148,\n",
       " 'compliment': 149,\n",
       " 'comrades': 150,\n",
       " 'concealing': 151,\n",
       " 'confirmed': 152,\n",
       " 'conscious': 153,\n",
       " 'considerate': 154,\n",
       " 'consumers': 155,\n",
       " 'corpse': 156,\n",
       " 'corruption': 157,\n",
       " 'could': 158,\n",
       " 'couldnt': 159,\n",
       " 'course': 160,\n",
       " 'covered': 161,\n",
       " 'create': 162,\n",
       " 'creature': 163,\n",
       " 'creeks': 164,\n",
       " 'crimson': 165,\n",
       " 'crystalline': 166,\n",
       " 'culprit': 167,\n",
       " 'cursed': 168,\n",
       " 'daggers': 169,\n",
       " 'dared': 170,\n",
       " 'dark': 171,\n",
       " 'darkness': 172,\n",
       " 'darted': 173,\n",
       " 'day': 174,\n",
       " 'days': 175,\n",
       " 'dead': 176,\n",
       " 'death': 177,\n",
       " 'debris': 178,\n",
       " 'decay': 179,\n",
       " 'deception': 180,\n",
       " 'deciding': 181,\n",
       " 'decision': 182,\n",
       " 'defect': 183,\n",
       " 'defecting': 184,\n",
       " 'deflected': 185,\n",
       " 'delicacy': 186,\n",
       " 'delicate': 187,\n",
       " 'demon': 188,\n",
       " 'diamond': 189,\n",
       " 'did': 190,\n",
       " 'didnt': 191,\n",
       " 'different': 192,\n",
       " 'differently': 193,\n",
       " 'digits': 194,\n",
       " 'disappeared': 195,\n",
       " 'disgruntle': 196,\n",
       " 'distance': 197,\n",
       " 'distribution': 198,\n",
       " 'divisions': 199,\n",
       " 'do': 200,\n",
       " 'dodged': 201,\n",
       " 'dome': 202,\n",
       " 'dont': 203,\n",
       " 'double': 204,\n",
       " 'doubted': 205,\n",
       " 'down': 206,\n",
       " 'dreariness': 207,\n",
       " 'dropped': 208,\n",
       " 'dropping': 209,\n",
       " 'drowning': 210,\n",
       " 'during': 211,\n",
       " 'each': 212,\n",
       " 'earth': 213,\n",
       " 'easily': 214,\n",
       " 'edible': 215,\n",
       " 'egg': 216,\n",
       " 'element': 217,\n",
       " 'else': 218,\n",
       " 'elsewhere': 219,\n",
       " 'embed': 220,\n",
       " 'emile': 221,\n",
       " 'emitting': 222,\n",
       " 'emotions': 223,\n",
       " 'emptiness': 224,\n",
       " 'encountered': 225,\n",
       " 'enemies': 226,\n",
       " 'enlightened': 227,\n",
       " 'enough': 228,\n",
       " 'eon': 229,\n",
       " 'equal': 230,\n",
       " 'equality': 231,\n",
       " 'equally': 232,\n",
       " 'escaped': 233,\n",
       " 'especially': 234,\n",
       " 'essence': 235,\n",
       " 'even': 236,\n",
       " 'events': 237,\n",
       " 'ever': 238,\n",
       " 'every': 239,\n",
       " 'everyone': 240,\n",
       " 'evidence': 241,\n",
       " 'evil': 242,\n",
       " 'exactly': 243,\n",
       " 'excited': 244,\n",
       " 'existed': 245,\n",
       " 'exposed': 246,\n",
       " 'eye': 247,\n",
       " 'eyes': 248,\n",
       " 'face': 249,\n",
       " 'failed': 250,\n",
       " 'familiar': 251,\n",
       " 'family': 252,\n",
       " 'far': 253,\n",
       " 'fauna': 254,\n",
       " 'favillain': 255,\n",
       " 'favillans': 256,\n",
       " 'feel': 257,\n",
       " 'feeling': 258,\n",
       " 'feelings': 259,\n",
       " 'feet': 260,\n",
       " 'fell': 261,\n",
       " 'felt': 262,\n",
       " 'fending': 263,\n",
       " 'fickle': 264,\n",
       " 'fight': 265,\n",
       " 'fill': 266,\n",
       " 'filled': 267,\n",
       " 'final': 268,\n",
       " 'finally': 269,\n",
       " 'find': 270,\n",
       " 'finger': 271,\n",
       " 'fingers': 272,\n",
       " 'fingertips': 273,\n",
       " 'fire': 274,\n",
       " 'first': 275,\n",
       " 'flame': 276,\n",
       " 'flesh': 277,\n",
       " 'flicker': 278,\n",
       " 'flitted': 279,\n",
       " 'flora': 280,\n",
       " 'flow': 281,\n",
       " 'flurry': 282,\n",
       " 'followed': 283,\n",
       " 'food': 284,\n",
       " 'for': 285,\n",
       " 'forest': 286,\n",
       " 'forests': 287,\n",
       " 'foriegn': 288,\n",
       " 'forsaken': 289,\n",
       " 'forth': 290,\n",
       " 'fortunate': 291,\n",
       " 'forward': 292,\n",
       " 'found': 293,\n",
       " 'fountain': 294,\n",
       " 'fragile': 295,\n",
       " 'freed': 296,\n",
       " 'fresher': 297,\n",
       " 'friend': 298,\n",
       " 'friendship': 299,\n",
       " 'friendships': 300,\n",
       " 'from': 301,\n",
       " 'fuel': 302,\n",
       " 'full': 303,\n",
       " 'fully': 304,\n",
       " 'furious': 305,\n",
       " 'further': 306,\n",
       " 'futile': 307,\n",
       " 'future': 308,\n",
       " 'galaxies': 309,\n",
       " 'gardens': 310,\n",
       " 'gave': 311,\n",
       " 'gaze': 312,\n",
       " 'gender': 313,\n",
       " 'get': 314,\n",
       " 'ghastly': 315,\n",
       " 'gift': 316,\n",
       " 'give': 317,\n",
       " 'glanced': 318,\n",
       " 'glass': 319,\n",
       " 'gleamed': 320,\n",
       " 'glistened': 321,\n",
       " 'glittered': 322,\n",
       " 'goal': 323,\n",
       " 'goals': 324,\n",
       " 'golden': 325,\n",
       " 'gotten': 326,\n",
       " 'grasps': 327,\n",
       " 'grassy': 328,\n",
       " 'greet': 329,\n",
       " 'greetings': 330,\n",
       " 'groupie': 331,\n",
       " 'grow': 332,\n",
       " 'guilt': 333,\n",
       " 'had': 334,\n",
       " 'hadnt': 335,\n",
       " 'hag': 336,\n",
       " 'half': 337,\n",
       " 'hall': 338,\n",
       " 'halt': 339,\n",
       " 'halted': 340,\n",
       " 'hand': 341,\n",
       " 'hands': 342,\n",
       " 'happened': 343,\n",
       " 'hare': 344,\n",
       " 'have': 345,\n",
       " 'he': 346,\n",
       " 'head': 347,\n",
       " 'healing': 348,\n",
       " 'heard': 349,\n",
       " 'heart': 350,\n",
       " 'height': 351,\n",
       " 'held': 352,\n",
       " 'hell': 353,\n",
       " 'her': 354,\n",
       " 'here': 355,\n",
       " 'hers': 356,\n",
       " 'him': 357,\n",
       " 'himself': 358,\n",
       " 'his': 359,\n",
       " 'hit': 360,\n",
       " 'hood': 361,\n",
       " 'hopes': 362,\n",
       " 'hour': 363,\n",
       " 'how': 364,\n",
       " 'hued': 365,\n",
       " 'hues': 366,\n",
       " 'hurt': 367,\n",
       " 'i': 368,\n",
       " 'idle': 369,\n",
       " 'idling': 370,\n",
       " 'if': 371,\n",
       " 'ignited': 372,\n",
       " 'im': 373,\n",
       " 'imbuement': 374,\n",
       " 'imitated': 375,\n",
       " 'imitator': 376,\n",
       " 'immediate': 377,\n",
       " 'important': 378,\n",
       " 'imprinted': 379,\n",
       " 'in': 380,\n",
       " 'inhabitants': 381,\n",
       " 'inhaled': 382,\n",
       " 'innocently': 383,\n",
       " 'inquiring': 384,\n",
       " 'inside': 385,\n",
       " 'inspecting': 386,\n",
       " 'instead': 387,\n",
       " 'intel': 388,\n",
       " 'intended': 389,\n",
       " 'intention': 390,\n",
       " 'intimate': 391,\n",
       " 'into': 392,\n",
       " 'irritated': 393,\n",
       " 'it': 394,\n",
       " 'item': 395,\n",
       " 'its': 396,\n",
       " 'itself': 397,\n",
       " 'jadus': 398,\n",
       " 'just': 399,\n",
       " 'kingdom': 400,\n",
       " 'knew': 401,\n",
       " 'knife': 402,\n",
       " 'knight': 403,\n",
       " 'knights': 404,\n",
       " 'know': 405,\n",
       " 'knowing': 406,\n",
       " 'knowledge': 407,\n",
       " 'known': 408,\n",
       " 'lack': 409,\n",
       " 'laid': 410,\n",
       " 'lake': 411,\n",
       " 'last': 412,\n",
       " 'later': 413,\n",
       " 'latters': 414,\n",
       " 'leafy': 415,\n",
       " 'leaned': 416,\n",
       " 'least': 417,\n",
       " 'leave': 418,\n",
       " 'leaving': 419,\n",
       " 'left': 420,\n",
       " 'less': 421,\n",
       " 'lettuce': 422,\n",
       " 'level': 423,\n",
       " 'levels': 424,\n",
       " 'libraries': 425,\n",
       " 'lie': 426,\n",
       " 'lied': 427,\n",
       " 'lies': 428,\n",
       " 'lifetime': 429,\n",
       " 'lifted': 430,\n",
       " 'light': 431,\n",
       " 'lightly': 432,\n",
       " 'like': 433,\n",
       " 'limp': 434,\n",
       " 'little': 435,\n",
       " 'locks': 436,\n",
       " 'long': 437,\n",
       " 'longer': 438,\n",
       " 'look': 439,\n",
       " 'looked': 440,\n",
       " 'looks': 441,\n",
       " 'loss': 442,\n",
       " 'lost': 443,\n",
       " 'lot': 444,\n",
       " 'lounged': 445,\n",
       " 'lower': 446,\n",
       " 'lowering': 447,\n",
       " 'luster': 448,\n",
       " 'mad': 449,\n",
       " 'made': 450,\n",
       " 'mages': 451,\n",
       " 'magic': 452,\n",
       " 'magical': 453,\n",
       " 'maidens': 454,\n",
       " 'making': 455,\n",
       " 'man': 456,\n",
       " 'mana': 457,\n",
       " 'mannerisms': 458,\n",
       " 'mans': 459,\n",
       " 'many': 460,\n",
       " 'mates': 461,\n",
       " 'me': 462,\n",
       " 'meant': 463,\n",
       " 'medium': 464,\n",
       " 'melt': 465,\n",
       " 'mere': 466,\n",
       " 'mess': 467,\n",
       " 'mightve': 468,\n",
       " 'mimicked': 469,\n",
       " 'mind': 470,\n",
       " 'minds': 471,\n",
       " 'mingling': 472,\n",
       " 'mirage': 473,\n",
       " 'miss': 474,\n",
       " 'missing': 475,\n",
       " 'moment': 476,\n",
       " 'month': 477,\n",
       " 'moonlight': 478,\n",
       " 'more': 479,\n",
       " 'most': 480,\n",
       " 'mountain': 481,\n",
       " 'mountains': 482,\n",
       " 'mouth': 483,\n",
       " 'moved': 484,\n",
       " 'much': 485,\n",
       " 'multitude': 486,\n",
       " 'nagging': 487,\n",
       " 'name': 488,\n",
       " 'natural': 489,\n",
       " 'nature': 490,\n",
       " 'need': 491,\n",
       " 'needing': 492,\n",
       " 'neither': 493,\n",
       " 'never': 494,\n",
       " 'nevierre': 495,\n",
       " 'nevierres': 496,\n",
       " 'new': 497,\n",
       " 'next': 498,\n",
       " 'night': 499,\n",
       " 'nights': 500,\n",
       " 'no': 501,\n",
       " 'noble': 502,\n",
       " 'noelles': 503,\n",
       " 'noelleâ': 504,\n",
       " 'noise': 505,\n",
       " 'none': 506,\n",
       " 'nor': 507,\n",
       " 'nose': 508,\n",
       " 'not': 509,\n",
       " 'nothing': 510,\n",
       " 'now': 511,\n",
       " 'of': 512,\n",
       " 'off': 513,\n",
       " 'often': 514,\n",
       " 'on': 515,\n",
       " 'once': 516,\n",
       " 'one': 517,\n",
       " 'ones': 518,\n",
       " 'ongoing': 519,\n",
       " 'only': 520,\n",
       " 'onto': 521,\n",
       " 'or': 522,\n",
       " 'order': 523,\n",
       " 'other': 524,\n",
       " 'others': 525,\n",
       " 'out': 526,\n",
       " 'outside': 527,\n",
       " 'outstretched': 528,\n",
       " 'over': 529,\n",
       " 'overpower': 530,\n",
       " 'own': 531,\n",
       " 'paid': 532,\n",
       " 'pain': 533,\n",
       " 'pale': 534,\n",
       " 'palm': 535,\n",
       " 'part': 536,\n",
       " 'passed': 537,\n",
       " 'past': 538,\n",
       " 'path': 539,\n",
       " 'paws': 540,\n",
       " 'pebble': 541,\n",
       " 'pebbles': 542,\n",
       " 'peeled': 543,\n",
       " 'performing': 544,\n",
       " 'perspective': 545,\n",
       " 'phantom': 546,\n",
       " 'physically': 547,\n",
       " 'pink': 548,\n",
       " 'pinning': 549,\n",
       " 'pinpoint': 550,\n",
       " 'pinpointed': 551,\n",
       " 'piqued': 552,\n",
       " 'place': 553,\n",
       " 'played': 554,\n",
       " 'playful': 555,\n",
       " 'plucked': 556,\n",
       " 'point': 557,\n",
       " 'pointed': 558,\n",
       " 'poisoning': 559,\n",
       " 'popped': 560,\n",
       " 'possible': 561,\n",
       " 'practice': 562,\n",
       " 'preached': 563,\n",
       " 'precision': 564,\n",
       " 'prepared': 565,\n",
       " 'presence': 566,\n",
       " 'prick': 567,\n",
       " 'prism': 568,\n",
       " 'pristine': 569,\n",
       " 'pronounced': 570,\n",
       " 'properties': 571,\n",
       " 'protector': 572,\n",
       " 'pull': 573,\n",
       " 'pulling': 574,\n",
       " 'pushed': 575,\n",
       " 'question': 576,\n",
       " 'quick': 577,\n",
       " 'rabbit': 578,\n",
       " 'radius': 579,\n",
       " 'raining': 580,\n",
       " 'ramblings': 581,\n",
       " 'rang': 582,\n",
       " 'rank': 583,\n",
       " 'ranks': 584,\n",
       " 'rattled': 585,\n",
       " 'raw': 586,\n",
       " 'reckless': 587,\n",
       " 'recognizing': 588,\n",
       " 'red': 589,\n",
       " 'reek': 590,\n",
       " 'references': 591,\n",
       " 'reflected': 592,\n",
       " 'remained': 593,\n",
       " 'replaced': 594,\n",
       " 'reserves': 595,\n",
       " 'resistant': 596,\n",
       " 'response': 597,\n",
       " 'rest': 598,\n",
       " 'retracted': 599,\n",
       " 'returned': 600,\n",
       " 'reveal': 601,\n",
       " 'rich': 602,\n",
       " 'riches': 603,\n",
       " 'riddled': 604,\n",
       " 'right': 605,\n",
       " 'ringing': 606,\n",
       " 'ripple': 607,\n",
       " 'river': 608,\n",
       " 'rivers': 609,\n",
       " 'romantic': 610,\n",
       " 'room': 611,\n",
       " 'rotten': 612,\n",
       " 'rotting': 613,\n",
       " 'run': 614,\n",
       " 'sake': 615,\n",
       " 'same': 616,\n",
       " 'sat': 617,\n",
       " 'saw': 618,\n",
       " 'scrolls': 619,\n",
       " 'searched': 620,\n",
       " 'seasonal': 621,\n",
       " 'seasons': 622,\n",
       " 'second': 623,\n",
       " 'seemed': 624,\n",
       " 'seemingly': 625,\n",
       " 'seen': 626,\n",
       " 'selecting': 627,\n",
       " 'sensed': 628,\n",
       " 'sensitive': 629,\n",
       " 'sensory': 630,\n",
       " 'set': 631,\n",
       " 'settling': 632,\n",
       " 'shadow': 633,\n",
       " 'shaken': 634,\n",
       " 'sharpen': 635,\n",
       " 'shattered': 636,\n",
       " 'shelves': 637,\n",
       " 'shift': 638,\n",
       " 'shoulder': 639,\n",
       " 'show': 640,\n",
       " 'shrank': 641,\n",
       " 'sigh': 642,\n",
       " 'similar': 643,\n",
       " 'simple': 644,\n",
       " 'sinful': 645,\n",
       " 'single': 646,\n",
       " 'singular': 647,\n",
       " 'sizable': 648,\n",
       " 'skewer': 649,\n",
       " 'sky': 650,\n",
       " 'sliver': 651,\n",
       " 'slung': 652,\n",
       " 'smaller': 653,\n",
       " 'smell': 654,\n",
       " 'smile': 655,\n",
       " 'sneak': 656,\n",
       " 'so': 657,\n",
       " 'societys': 658,\n",
       " 'solar': 659,\n",
       " 'solid': 660,\n",
       " 'some': 661,\n",
       " 'someone': 662,\n",
       " 'soon': 663,\n",
       " 'soul': 664,\n",
       " 'source': 665,\n",
       " 'space': 666,\n",
       " 'spade': 667,\n",
       " 'spanning': 668,\n",
       " 'spark': 669,\n",
       " 'speaking': 670,\n",
       " 'spear': 671,\n",
       " 'spears': 672,\n",
       " 'speed': 673,\n",
       " 'spell': 674,\n",
       " 'spent': 675,\n",
       " 'spewed': 676,\n",
       " 'spinning': 677,\n",
       " 'spring': 678,\n",
       " 'squadron': 679,\n",
       " 'standing': 680,\n",
       " 'starry': 681,\n",
       " 'stars': 682,\n",
       " 'statement': 683,\n",
       " 'statue': 684,\n",
       " 'steady': 685,\n",
       " 'steal': 686,\n",
       " 'stealing': 687,\n",
       " 'stepping': 688,\n",
       " 'still': 689,\n",
       " 'sting': 690,\n",
       " 'stir': 691,\n",
       " 'stone': 692,\n",
       " 'stood': 693,\n",
       " 'stratagems': 694,\n",
       " 'stretched': 695,\n",
       " 'strong': 696,\n",
       " 'stronger': 697,\n",
       " 'subdued': 698,\n",
       " 'suddenly': 699,\n",
       " 'summon': 700,\n",
       " 'sun': 701,\n",
       " 'sunlight': 702,\n",
       " 'surprises': 703,\n",
       " 'suspicious': 704,\n",
       " 'swear': 705,\n",
       " 'sweep': 706,\n",
       " 'sweeter': 707,\n",
       " 'swim': 708,\n",
       " 'systems': 709,\n",
       " 'target': 710,\n",
       " 'taste': 711,\n",
       " 'tasting': 712,\n",
       " 'taunting': 713,\n",
       " 'tearing': 714,\n",
       " 'terrible': 715,\n",
       " 'than': 716,\n",
       " 'that': 717,\n",
       " 'thats': 718,\n",
       " 'the': 719,\n",
       " 'theft': 720,\n",
       " 'their': 721,\n",
       " 'them': 722,\n",
       " 'themselves': 723,\n",
       " 'then': 724,\n",
       " 'there': 725,\n",
       " 'they': 726,\n",
       " 'thief': 727,\n",
       " 'thieving': 728,\n",
       " 'thing': 729,\n",
       " 'think': 730,\n",
       " 'thinking': 731,\n",
       " 'this': 732,\n",
       " 'thorn': 733,\n",
       " 'those': 734,\n",
       " 'though': 735,\n",
       " 'thought': 736,\n",
       " 'thoughts': 737,\n",
       " 'threatened': 738,\n",
       " 'threatening': 739,\n",
       " 'thresholds': 740,\n",
       " 'through': 741,\n",
       " 'time': 742,\n",
       " 'to': 743,\n",
       " 'toes': 744,\n",
       " 'tomes': 745,\n",
       " 'tongue': 746,\n",
       " 'tonight': 747,\n",
       " 'too': 748,\n",
       " 'took': 749,\n",
       " 'torturous': 750,\n",
       " 'tossing': 751,\n",
       " 'touch': 752,\n",
       " 'towering': 753,\n",
       " 'traced': 754,\n",
       " 'traces': 755,\n",
       " 'trained': 756,\n",
       " 'traitor': 757,\n",
       " 'travel': 758,\n",
       " 'treat': 759,\n",
       " 'treats': 760,\n",
       " 'trickling': 761,\n",
       " 'trickster': 762,\n",
       " 'tried': 763,\n",
       " 'trivial': 764,\n",
       " 'troubles': 765,\n",
       " 'true': 766,\n",
       " 'trust': 767,\n",
       " 'tugged': 768,\n",
       " 'tulips': 769,\n",
       " 'turmoil': 770,\n",
       " 'turned': 771,\n",
       " 'twisted': 772,\n",
       " 'twitching': 773,\n",
       " 'two': 774,\n",
       " 'typical': 775,\n",
       " 'unbeknownst': 776,\n",
       " 'under': 777,\n",
       " 'underground': 778,\n",
       " 'underneath': 779,\n",
       " 'understanding': 780,\n",
       " 'unexpected': 781,\n",
       " 'unknown': 782,\n",
       " 'unknowns': 783,\n",
       " 'unnaturally': 784,\n",
       " 'until': 785,\n",
       " 'up': 786,\n",
       " 'upon': 787,\n",
       " 'useful': 788,\n",
       " 'usual': 789,\n",
       " 'usually': 790,\n",
       " 'velvet': 791,\n",
       " 'vicinity': 792,\n",
       " 'victim': 793,\n",
       " 'voice': 794,\n",
       " 'wait': 795,\n",
       " 'wake': 796,\n",
       " 'want': 797,\n",
       " 'war': 798,\n",
       " 'warmth': 799,\n",
       " 'warred': 800,\n",
       " 'was': 801,\n",
       " 'wasnt': 802,\n",
       " 'watched': 803,\n",
       " 'way': 804,\n",
       " 'ways': 805,\n",
       " 'weight': 806,\n",
       " 'were': 807,\n",
       " 'werent': 808,\n",
       " 'what': 809,\n",
       " 'when': 810,\n",
       " 'where': 811,\n",
       " 'which': 812,\n",
       " 'whimsical': 813,\n",
       " 'white': 814,\n",
       " 'who': 815,\n",
       " 'whole': 816,\n",
       " 'why': 817,\n",
       " 'will': 818,\n",
       " 'willed': 819,\n",
       " 'wind': 820,\n",
       " 'wings': 821,\n",
       " 'wished': 822,\n",
       " 'witching': 823,\n",
       " 'with': 824,\n",
       " 'within': 825,\n",
       " 'woman': 826,\n",
       " 'wordy': 827,\n",
       " 'work': 828,\n",
       " 'working': 829,\n",
       " 'workings': 830,\n",
       " 'world': 831,\n",
       " 'worse': 832,\n",
       " 'would': 833,\n",
       " 'wouldnt': 834,\n",
       " 'wrapper': 835,\n",
       " 'wrecked': 836,\n",
       " 'years': 837,\n",
       " 'yes': 838,\n",
       " 'yet': 839,\n",
       " 'you': 840,\n",
       " 'younger': 841,\n",
       " 'your': 842,\n",
       " 'youth': 843,\n",
       " 'zack': 844}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping words (???)\n",
    "words = text.split()\n",
    "words = sorted(list(set(words)))\n",
    "rawWords = []\n",
    "for word in words:\n",
    "    lettersOnly = ''.join(filter(str.isalpha, word))\n",
    "    rawWords.append(lettersOnly)\n",
    "\n",
    "rawWords = sorted(list(set(rawWords)))\n",
    "\n",
    "nToWord = { n:word for n, word in enumerate(rawWords) }\n",
    "wordToN = { word:n for n, word in enumerate(rawWords) }\n",
    "\n",
    "wordToN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pre-processing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "length = len(text)\n",
    "seq_length = 100\n",
    "for i in range(0, length-seq_length, 1):\n",
    "    sequence = text[i:i + seq_length]\n",
    "    label =text[i + seq_length]\n",
    "    x.append([charToN[char] for char in sequence])\n",
    "    y.append(charToN[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xMod = np.reshape(x, (len(x), seq_length, 1))\n",
    "xMod = xMod / float(len(characters))\n",
    "yMod = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modeling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(700, input_shape=(xMod.shape[1], xMod.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(700, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(700))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(yMod.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  200/12066 [..............................] - ETA: 13:07 - loss: 3.6623"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-92ae5356af74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxMod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myMod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(xMod, yMod, epochs=1, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('text_generator_gigantic.h5')\n",
    "model.load_weights('text_generator_gigantic.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text Generation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original prediction [[5.0222483e-03 1.9426273e-01 1.9882315e-04 2.1288788e-03 4.5147962e-03\n",
      "  7.6511833e-03 7.8870071e-05 8.4973127e-03 9.7125150e-05 1.5485795e-04\n",
      "  1.6255291e-04 1.2941032e-03 7.4337035e-02 9.0993810e-03 1.6253086e-02\n",
      "  3.6185525e-02 1.1295947e-01 2.2133233e-02 2.3589419e-02 4.1377962e-02\n",
      "  5.2495722e-02 5.5499724e-04 4.7752475e-03 2.7399700e-02 2.0823453e-02\n",
      "  6.5756783e-02 4.5512524e-02 8.4797097e-03 1.2220471e-03 2.9433718e-02\n",
      "  3.7698381e-02 8.3695613e-02 2.5924839e-02 4.6668779e-03 1.8472821e-02\n",
      "  9.2570100e-04 9.2636365e-03 5.3003355e-04 1.6590480e-04 2.0995684e-04\n",
      "  8.1789412e-04 2.3854841e-04 6.8820571e-04 2.4909154e-04]]\n",
      "np.argmax 1\n",
      "string_mapped [12, 30, 30, 16, 15, 1, 13, 36, 1, 31, 19, 16, 24, 1, 34, 20, 31, 19, 7, 1, 31, 19, 16, 20, 29, 1, 34, 12, 29, 1, 34, 20, 31, 19, 1, 15, 20, 12, 24, 26, 25, 15, 1, 19, 12, 15, 1, 14, 16, 12, 30, 16, 15, 5, 1, 36, 16, 31, 1, 31, 32, 29, 24, 26, 20, 23, 1, 29, 16, 24, 12, 20, 25, 16, 15, 1, 34, 20, 31, 19, 20, 25, 1, 14, 23, 26, 33, 16, 29, 1, 34, 20, 31, 19, 1, 31, 19, 16, 1, 15]\n",
      "character for each value in string_mapped ['a', 's', 's', 'e', 'd', ' ', 'b', 'y', ' ', 't', 'h', 'e', 'm', ' ', 'w', 'i', 't', 'h', '.', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'w', 'a', 'r', ' ', 'w', 'i', 't', 'h', ' ', 'd', 'i', 'a', 'm', 'o', 'n', 'd', ' ', 'h', 'a', 'd', ' ', 'c', 'e', 'a', 's', 'e', 'd', ',', ' ', 'y', 'e', 't', ' ', 't', 'u', 'r', 'm', 'o', 'i', 'l', ' ', 'r', 'e', 'm', 'a', 'i', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', 'i', 'n', ' ', 'c', 'l', 'o', 'v', 'e', 'r', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'd']\n",
      "character for prediction  \n",
      "string mapped [30, 30, 16, 15, 1, 13, 36, 1, 31, 19, 16, 24, 1, 34, 20, 31, 19, 7, 1, 31, 19, 16, 20, 29, 1, 34, 12, 29, 1, 34, 20, 31, 19, 1, 15, 20, 12, 24, 26, 25, 15, 1, 19, 12, 15, 1, 14, 16, 12, 30, 16, 15, 5, 1, 36, 16, 31, 1, 31, 32, 29, 24, 26, 20, 23, 1, 29, 16, 24, 12, 20, 25, 16, 15, 1, 34, 20, 31, 19, 20, 25, 1, 14, 23, 26, 33, 16, 29, 1, 34, 20, 31, 19, 1, 31, 19, 16, 1, 15]\n"
     ]
    }
   ],
   "source": [
    "# This is here because we need somewhere to start before the model can start predicting the next words/characters\n",
    "# So then wouldn't it make more sense if I tried to map the start by word instead of characters...\n",
    "string_mapped = x[98]\n",
    "full_string = [nToChar[value] for value in string_mapped]\n",
    "\n",
    "xPred = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "xPred = xPred / float(len(characters))\n",
    "\n",
    "predict = model.predict(xPred)\n",
    "print('original prediction', predict)\n",
    "predict = np.argmax(predict)\n",
    "print('np.argmax', predict)\n",
    "seq = [nToChar[value] for value in string_mapped]\n",
    "print('string_mapped', string_mapped)\n",
    "print('character for each value in string_mapped', seq)\n",
    "fullString = nToChar[predict]\n",
    "print('character for prediction', fullString)\n",
    "\n",
    "string_mapped = string_mapped[1:len(string_mapped)]\n",
    "print('string mapped', string_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_1_input to have shape (100, 1) but got array with shape (101, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-be35bbef547c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mxPred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxPred\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcharacters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpred_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxPred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mseq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnToChar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_mapped\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_1_input to have shape (100, 1) but got array with shape (101, 1)"
     ]
    }
   ],
   "source": [
    "string_mapped = x[99]\n",
    "full_string = [nToChar[value] for value in string_mapped]\n",
    "\n",
    "# generating characters\n",
    "for i in range(400):\n",
    "    xPred = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "    xPred = xPred / float(len(characters))\n",
    "\n",
    "    pred_index = np.argmax(model.predict(xPred, verbose=0))\n",
    "    print(pred_index)\n",
    "    seq = [nToChar[value] for value in string_mapped]\n",
    "    full_string.append(nToChar[pred_index])\n",
    "\n",
    "    string_mapped.append(pred_index)\n",
    "    string_mapped = string_mapped[1:len(string_mapped)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining text\n",
    "txt=\"\"\n",
    "for char in full_string:\n",
    "    txt = txt+char\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
